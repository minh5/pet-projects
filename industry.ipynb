{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-25T11:51:18.150237",
     "start_time": "2017-01-25T11:51:05.439923"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, auc, brier_score_loss, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "The task is to classify which industry a company belongs to given the name of a company. The dataset contains approximately 300,000 companies, spanning 99 industries. The dataset only contains two columns which is the name and the true proper classification of the industry it belongs to. This is unique because a) rather than having a wide dataset, the dataset is small in terms of features and b) conventional methods of data analysis would not apply since I do not have enough features and would have to engineer them myself.\n",
    "\n",
    "The steps I took to address this issue are:\n",
    "    * Generate the features\n",
    "    * Transform them to be meaningful\n",
    "    * Eliminate redundant or noisy features\n",
    "    * Train the data and assess the model\n",
    "    \n",
    "To begin, here is a quick look at the data set that was given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A - E Employees Credit Union</td>\n",
       "      <td>Diversified Financial Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A &amp; A Contract Services, Inc.</td>\n",
       "      <td>Professional Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A &amp; A Express, Inc.</td>\n",
       "      <td>Road and Rail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A &amp; A Fertilizer, Ltd.</td>\n",
       "      <td>Chemicals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A &amp; A Food Service. Inc.</td>\n",
       "      <td>Distributors</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Company                        Industry\n",
       "0   A - E Employees Credit Union  Diversified Financial Services\n",
       "1  A & A Contract Services, Inc.           Professional Services\n",
       "2            A & A Express, Inc.                   Road and Rail\n",
       "3         A & A Fertilizer, Ltd.                       Chemicals\n",
       "4       A & A Food Service. Inc.                    Distributors"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/cipherpol/Downloads/private_us_companies.csv',\n",
    "                   encoding='latin1', usecols=['Company', 'Industry'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation\n",
    "\n",
    "Since the dataset lacks features to describe the classification, I would have to generate some. My first thought is to parse the name and hopefully find some meaningful patterns there, but a quick glance would show this would not work since  there are similar names such as `A & A Express, Inc` and `A & A Fertilizer, Ltd` which have `A & A` but are classified differently. For instance, I have no theoretical reason to believe that names that start with A would be more likely to be classified as Chemicals for instance.\n",
    "\n",
    "Usually, the first thing most people do when faced with unfamiliar information is to Google that concept. I was able to find a website for most companies here and decided to go down this approach in order to generate features. I used the Google Custom Search API. The idea was to create a custom search using the [API](https://developers.google.com/custom-search/docs/tutorial/creatingcse) and search the company name. From there I would pick the most relevant search according to their PageRank algorithm and grabed the `htmlSnippet` and `htmlTitle`. The reason for this approach is that the `htmlSnippet` would describe the body of text for the company while the `htmlTitle` would be a quick summary of the company. Both of these are highly relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I saved my api and custom search engine id (cse_id) and retrieved them from my environmental variables for security purposes, then I created a quick function to parse it.\n",
    "\n",
    "    api_key = os.environ.get('google_api_key')\n",
    "    cse_id = os.environ.get('cse_id')\n",
    "\n",
    "    # custom search using the Google Search API\n",
    "    def google_search(search_term, api_key, cse_id, **kwargs):\n",
    "        service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "        res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()\n",
    "        result = res['items']\n",
    "        return result[0]['htmlSnippet'] + \" \" + result[0]['htmlTitle']\n",
    "\n",
    "\n",
    "An example is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<b>A &amp; A Food Service Company</b> is a full service purveyor to the food industry. Our <br>\\nclientele includes restaurants, convention centers, hospitals, high-end hotels,&nbsp;... <b>A &amp; A FOODSERVICE</b> - Welcome'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_search('A & A Food Service. Inc.', my_api_key, my_cse_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, I simply just pass it through some regex filters to clean up any HTML tags along with any unicode characters present and saved it as a dictionary for future processing.\n",
    "\n",
    "    def clean_text(text):\n",
    "        remove_tags = re.compile('(\\<\\/?\\w+\\>)')\n",
    "        tags_gone= remove_tags.sub(' ', text)\n",
    "        clean_up = re.compile('(&#39;_)|(\\W+)')\n",
    "        return clean_up.sub(' ', tags_gone).strip()\n",
    "        \n",
    "(**Disclaimer: The Google CSE API only allows 100 API requests a day and charges $5 for every** **1,000 queries up to 10,000 a day. Given these financial constraints, I only used a sample  of 100 companies to create this report**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "Now that I have the text parsed and cleaned, I have to extract the features and convert it in a way that can be useful. My approach is as followed:\n",
    "   * Count the occurrence of all the words\n",
    "   * Create an array that represents the presence of a word for each company\n",
    "   * Transform the array to represent the importance of the word\n",
    "  \n",
    "To accomplish this, I used the `CountVectorizer` which takes in a corpus of text and then with each company's description, you can get an array that contain the counts of each word. For instance, if the corpus is this (taken from the sklearn [documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage))\n",
    "\n",
    "    corpus = [\n",
    "        'This is the first document.',\n",
    "        'This is the second second document.',\n",
    "        'And the third one.',\n",
    "        'Is this the first document?',\n",
    "    ]\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "will be converted to\n",
    "\n",
    "    array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
    "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
    "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
    "       [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)\n",
    "       \n",
    "where the position of the numerical values correspond to each\n",
    " \n",
    "     vectorizer.get_feature_names()\n",
    "     (['and', 'document', 'first', 'is', 'one',\n",
    "       'second', 'the', 'third', 'this']\n",
    "      )\n",
    " \n",
    "So in the first array, we see a `0` in the first element because there is no `and` but there is `document`, `first`, and `is` so the second, third, and fourth element in the array is represented with a `1`. From here, I simply just pass in the values of my dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have vectorize the word counts, I wanted to re-weigh them. I used the [Tfâ€“idf term](http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting) method. The idea behind this technique is that words that are common such as `a` or `the` will have a lesser weights than words that are rare such as `Agriculture` or `Banking`. My theory is that words that are rarer will also be correlated with the group they belong to and I wanted to capture that relationship.\n",
    "\n",
    "Below I ran a quick example of my theory and we can see the shape of the vector names and the word vector match up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "inv_transformer = TfidfTransformer(smooth_idf=False)\n",
    "vec = count_vectorizer.fit_transform(self.words.values()).toarray()\n",
    "vector = inv_transformer.fit_transform(vec).toarray()\n",
    "vector_names = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 696)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.shape # a 50 x 696 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "696"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['electronic', 'cycling', 'around', 'jones', 'should', '2005', 'pty',\n",
       "       'union', 'bamboo', '944', 'southwest', 'contact', 'kansas',\n",
       "       'apartment', 'collins', 'huneeus', 'cases', 'york', 'holiday',\n",
       "       'networks'], \n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a random selection of words\n",
    "np.random.choice(np.array(self.vector_names), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feature Selection\n",
    "\n",
    "From the `CountVectorizer` we have a wide variety of words that describes these companies. This presents me with a problem that was the opposite at the beginning: we now have too many features and our data set is sparse. This means that the data set is populated with many 0 which makes sense because each word is drawn from a large corpus. With a sample of 50 companies, we have approximately 700 features represented.\n",
    "\n",
    "Now my task is to reduce the values. To do this, I used two techniques: an ElasticNet regularizator and Principle Component Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why I chose an [ElasticNet](https://web.stanford.edu/~hastie/Papers/B67.2%20(2005/\\%\\20301-320%20Zou%20&%20Hastie.pdf) is because it's a compromise between L1 and L2 regularization techiques. These regularizations places a penalty on wide feature sets and enforces parsimony of features. L1 regularization will find a group of variables that are correlated and randomly select one, while L2 will simply shrink the coefficients but never make them zero, thus preserving the structure of the current features. Elastic Net compromises between the two. The reason I chose this techinque is because I believe there is a grouping effect of variables. For instance, the work \"Banking, Investment, and Credit\" might indicate that it is a financial service industry.\n",
    "\n",
    "The ElasticNet regularization can be called using the [SelectFromModel](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) wrapper, which allows feature selection by models that have feature importance rankings\n",
    "\n",
    "            select = SelectFromModel(ElasticNet())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I just implemented the code. First I ran a quick `LabelEncoder` that would assign a unique value for every industry (since the outcome cannot be a string) and then I scaled each column vector to a mean of 0 and a standard deviation of 1. We see that the 696 columns down to 157 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 696)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelizer = LabelEncoder()\n",
    "labelizer.fit(list(set(self.df.Industry)))\n",
    "X, self.y = self.df.ix[:, 1:-1], labelizer.transform(self.df.Industry)\n",
    "scalar = StandardScaler()\n",
    "scaled = scalar.fit_transform(X)\n",
    "scaled.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 157)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select = SelectFromModel(ElasticNet())\n",
    "selected = select.fit_transform(scaled, self.y)\n",
    "selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part is to run [Principle Component Analysis](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA) which will reduce the features into components that captures the highest variance in the data. The idea is to reduce the features while retaining variance. This part is depending on the client need. If the client would prefer interpretable features, then I would omit using PCA. However, if the concern is parsimony and predictive power, I would select this technique. Furthermore, reducing the number of features would improve computational speed, which would be useful as we scale up our analysis.\n",
    "\n",
    "In this scenario, I did not specify the number of components, but I allow it to iterate until the projected components represent at least 95% of the variance of the original data set.\n",
    "\n",
    "    pca = PCA(n_components=.95, svd_solver='full')\n",
    "    \n",
    "Here we see that we further reduced the variables from 157 to 15 that captures 95% of the variance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 15)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=.90, svd_solver='full')\n",
    "decomposed = self.X = pca.fit_transform(selected)\n",
    "decomposed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "After feature selection, I can turn to the modeling process. I chose to implement a fairly popular model: [The Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). The reason I chose this model is that\n",
    "    * it is robust to outliers,\n",
    "    * I am assuming there is nonlinearity within the data\n",
    "    * it is less likely to overfit since it only sample certain features each iteration\n",
    "    * it **scales** every well since the bootstrapping can occur on separate nodes\n",
    "\n",
    "In addition to these benefits, I can also implement a hyperparameter tuning methods to determine the best combination of parameters to pass into the Random Forest algorithm that would yield the lowest cross-validated error rates. This can be done by using the [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), where I can specify the parameters and values into a dictionary to be searched through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "     'n_estimators': [2, 3, 4, 5, 6],\n",
    "     'max_features': [1, 2, 3, 4],\n",
    "     'criterion': ['gini', 'entropy']}\n",
    "rf = GridSearchCV(\n",
    "     estimator=RandomForestClassifier(),\n",
    "     param_grid=params,\n",
    "     n_jobs=3,\n",
    "     verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here the tuning grid will search through `n_estimators`, `max_features`, and `criterion` parameter and their respective values, which will yield 40 different combinations.\n",
    "\n",
    "After training the model, depending on the business case, we could use several metrics. We can train the model to maximize specificity or sensitivity. I opted out from running a metrics examination because the training (and the test set) are small due to the API limit and I do not know the use case of the model. Do we care more about labeling industries correctly or minimizing wrong classifications? How do we intend to use the model? What subject-matter expertise can I draw from to improve the model? These are all important questions I ask before I decide on the proper metric to use for model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-25T11:58:47.529400",
     "start_time": "2017-01-25T11:58:47.525859"
    },
    "collapsed": false
   },
   "source": [
    "Putting everything together, we can create two classes: one for generating and parsing the features and one for extracting, transforming, and training. The reason why I create these classes because it will allow for changing states in our training process and train models with just two lines of code. Also, it will be easier to scale up if we build out into a web application (Django, for example).\n",
    "\n",
    "\n",
    "```\n",
    "class Parser:\n",
    "\n",
    "    def __init__(self, list_of_words, api_key, cse_id):\n",
    "        self.words = list_of_words\n",
    "        self.api = api_key\n",
    "        self.cse = cse_id\n",
    "\n",
    "    @staticmethod\n",
    "    def google_search(search_term, api_key, cse_id, **kwargs):\n",
    "        service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "        res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()\n",
    "        result = res['items']\n",
    "        return result[0]['htmlSnippet'] + \" \" + result[0]['htmlTitle']\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        remove_tags = re.compile('(\\<\\/?\\w+\\>)')\n",
    "        tags_gone= remove_tags.sub(' ', text)\n",
    "        clean_up = re.compile('(&#39;_)|(\\W+)')\n",
    "        return clean_up.sub(' ', tags_gone).strip()\n",
    "\n",
    "    def run(self):\n",
    "        search_dict = {}\n",
    "        for word in self.words:\n",
    "            result = self.google_search(word, self.api, self.cse)\n",
    "            cleaned = self.clean_text(result)\n",
    "            search_dict[word] = cleaned\n",
    "        self.word_dict = search_dict\n",
    "\n",
    "\n",
    "class Analyzer:\n",
    "\n",
    "    def __init__(self, word_dict, data):\n",
    "        self.words = word_dict\n",
    "        self.raw_df = data\n",
    "\n",
    "    def prepare_feature_vector(self):\n",
    "        count_vectorizer = CountVectorizer()\n",
    "        inv_transformer = TfidfTransformer(smooth_idf=False)\n",
    "        vec = count_vectorizer.fit_transform(self.words.values()).toarray()\n",
    "        self.vector = inv_transformer.fit_transform(vec).toarray()\n",
    "        self.vector_names = count_vectorizer.get_feature_names()\n",
    "\n",
    "    def prepare_df(self):\n",
    "        feat_df = pd.DataFrame(self.vector, columns=self.vector_names)\n",
    "        company_df = pd.DataFrame(list(self.words.keys()))\n",
    "        full_df = pd.concat([company_df, feat_df], ignore_index=True, axis=1)\n",
    "        full_df.columns = ['Company'] + self.vector_names\n",
    "        self.df = pd.merge(full_df, self.raw_df)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        labelizer = LabelEncoder()\n",
    "        labelizer.fit(list(set(self.df.Industry)))\n",
    "        X, self.y = self.df.ix[:, 1:-1], labelizer.transform(self.df.Industry)\n",
    "        scalar = StandardScaler()\n",
    "        scaled = scalar.fit_transform(X)\n",
    "        select = SelectFromModel(ElasticNet())\n",
    "        selected = select.fit_transform(X, self.y)\n",
    "        pca = PCA(n_components=.90, svd_solver='full')\n",
    "        self.X = pca.fit_transform(selected)\n",
    "\n",
    "    def train_data(self, **kwargs):\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.X, self.y, test_size=0.2, random_state=5)\n",
    "        params = {\n",
    "            'n_estimators': [2, 3, 4, 5, 6],\n",
    "            'max_features': [1, 2, 3, 4],\n",
    "            'criterion': ['gini', 'entropy']\n",
    "        }\n",
    "        if kwargs:\n",
    "            for key, value in kwargs.items():\n",
    "                params[key] = value\n",
    "        self.rf = GridSearchCV(\n",
    "            estimator=RandomForestClassifier(),\n",
    "            param_grid=params,\n",
    "            n_jobs=3,\n",
    "            verbose=True)\n",
    "        self.rf.fit(self.x_train, self.y_train)\n",
    "        self.class_predict = self.rf.predict(self.x_test)\n",
    "        self.probs_predict = self.rf.predict_proba(self.x_test)[:, 1]\n",
    "        \n",
    "    def get_metrics(self):\n",
    "        self.report = classification_report(self.y_test, self.class_predict, \n",
    "        self.vector_names)\n",
    "        \n",
    "    def run(self):\n",
    "        self.prepare_feature_vector()\n",
    "        self.prepare_df()\n",
    "        self.preprocess_data()\n",
    "        self.train_data()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are limitations that I wished to address. This project only assumes that the Google Search terms are correlated to the industry classification, which I believe is a reasonable assumption. And without being able to fully grab all the necessary data possible, I was not able to explore the data a bit more and build out a more robust model.\n",
    "\n",
    "Other approaches I could have employed if I had more time:\n",
    "\n",
    "  * Exploring the use of bi-grams and  tri-grams and seeing if they improve model performance\n",
    "  * Appending census data once we are able to parse out the location and address of the business\n",
    "  * Implementing a [Word2Vec](https://arxiv.org/pdf/1301.3781v3.pdf) algorithm  to understand the relationship between certain words\n",
    "  * Using a [TruncatedSVD](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD) to decompose a spare matrix (I did not implement because I do not understand it fully)\n",
    "  * Using the [XGBOOST](https://github.com/dmlc/xgboost) Model for classification, since it is one of the best supervised learning algorithms used (I did not implement since I wanted to stay inside the Sklearn framework)\n",
    "  * Implementing a recurrent neural network to help with text classification since the location and context of the world most likely will have an impact on industry classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
